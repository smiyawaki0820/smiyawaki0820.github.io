class Args:
    # Path
    path = None  # model path
    spm = None  # tokenizer path
    data = None  # vocab

    # For neural netwotk model parameters
    all_gather_list_size = 16384
    beam = 5
    bf16 = False
    bpe = None
    broadcast_buffers = False
    bucket_cap_mb = 25
    buffer_size = 0
    checkpoint_suffix = ''
    cpu = False
    criterion = 'cross_entropy'
    data_buffer_size = 10
    dataset_impl = None
    ddp_backend = 'c10d'
    decoding_format = None
    device_id = 0
    distributed_backend = 'nccl'
    distributed_init_method = None
    distributed_no_spawn = False
    distributed_port = -1
    distributed_rank = 0
    distributed_world_size = 1
    distributed_wrapper = 'DDP'
    diverse_beam_groups = 5
    diverse_beam_strength = 2.0
    diversity_rate = -1.0
    empty_cache_freq = 0
    eval_bleu = False
    eval_bleu_args = None
    eval_bleu_detok = 'space'
    eval_bleu_detok_args = None
    eval_bleu_print_samples = False
    eval_bleu_remove_bpe = None
    eval_tokenized_bleu = False
    fast_stat_sync = False
    find_unused_parameters = False
    fix_batches_to_gpus = False
    force_anneal = None
    fp16 = False
    fp16_init_scale = 128
    fp16_no_flatten_grads = False
    fp16_scale_tolerance = 0.0
    fp16_scale_window = None
    gen_subset = 'test'
    input = '-'
    iter_decode_eos_penalty = 0.0
    iter_decode_force_max_iter = False
    iter_decode_max_iter = 10
    iter_decode_with_beam = 1
    iter_decode_with_external_reranker = False
    left_pad_source = 'True'
    left_pad_target = 'False'
    lenpen = 1
    load_alignments = False
    localsgd_frequency = 3
    log_format = None
    log_interval = 100
    lr_scheduler = 'fixed'
    lr_shrink = 0.1
    match_source_len = False
    max_len_a = 0
    max_len_b = 30
    max_sentences = None
    max_source_positions = 1024
    max_target_positions = 1024
    max_tokens = 2000
    memory_efficient_bf16 = False
    memory_efficient_fp16 = False
    min_len = 5
    min_loss_scale = 0.0001
    model_overrides = '{}'
    model_parallel_size = 1
    momentum = 0.99
    nbest = 5
    no_beamable_mm = False
    no_early_stop = False
    no_progress_bar = False
    no_repeat_ngram_size = 2
    no_seed_provided = True
    nprocs_per_node = 2
    num_batch_buckets = 0
    num_shards = 1
    num_workers = 1
    optimizer = 'nag'
    prefix_size = 0
    print_alignment = False
    print_step = False
    profile = False
    quantization_config_path = None
    quiet = False
    remove_bpe = None
    replace_unk = None
    required_batch_size_multiple = 8
    results_path = None
    retain_dropout = False
    retain_dropout_modules = None
    retain_iter_history = False
    sacrebleu = False
    sampling = False
    sampling_topk = -1
    sampling_topp = -1.0
    score_reference = False
    seed = 1
    shard_id = 0
    skip_invalid_size_inputs_valid_test = False
    slowmo_algorithm = 'LocalSGD'
    slowmo_momentum = None
    source_lang = 'context'
    target_lang = 'response'
    task = 'translation'
    temperature = 1.0
    tensorboard_logdir = ''
    threshold_loss_scale = None
    tokenizer = None
    tpu = False
    truncate_source = False
    unkpen = 0
    unnormalized = False
    upsample_primary = 1
    user_dir = None
    warmup_updates = 0
    weight_decay = 0.0

    def __init__(self, model_path: str = None, spm_path: str = None, vocab_path: str = None):
        self.path = model_path
        self.spm = spm_path
        self.data = vocab_path
